第1章 特征工程
01 为什么要对数值类型的特征做归一化？
梯度下降时，数值类型特征范围的大区间等于更大的随机波动。
011 有哪些归一化的方法，适用什么场景？
线性函数归一化（Min-Max Scaling）（将原始数据映射到[0,1]的范围）：不考虑方差差异，适用图像处理。
以及零均值归一化（Z-Score Normalizaation）（将原始数据映射到均值为0、标准差为1的分布上）：分类、聚类算法，PCA降维。
012 什么场景不需要归一化？
不进行梯度下降的场景，比如xgboost。

02 怎样处理类别型特征？
序号编码、独热编码、二进制编码
021 应用场景？
序号编号时随机分派0，1，2；独热只考虑单类别，二进制适用huffman编码（二叉树）。

03 什么是组合特征？如何处理高维组合特征？
特征组合，乘法组合。

04 怎样高效地找到组合特征？
串行xgboost，并行随机森林。

05 有那些文本表示模型？它们各有什么优缺点？
无上下文无词序：词袋模型和N-gram模型（长度为N的词）
TF−IDF(t,d)=TF(t,d)∗IDF(t)。TF(t,d)为单词t在文档d中的频率，IDF(t)=log(文章总数/包含单词t的文章总数+1)。
主题模型（LSI）：奇异值分解三矩阵。
有上下文无词序：W2c
有上下文有词序：bert，xlnet

06 Word2Vec 是如何工作的？它和隐狄利克雷模型（LDA）有什么区别与联系？
Word2Vec：上下文词向量平均=本词向量。
LDA：是‘文档-单词’矩阵进行分解,获得主题分布，概率图模型的生成式模型。

07 如何缓解图像分类任务中训练数据不足带来的问题？
一、基于模型的方法，包括简化模型、添加正则项以缩小假设空间、集成学习、Dropout超参数；
二、基于数据的方法，主要通过数据扩充（Data Augmentation），例如图像增强、SMOTE算法、生成式对抗网络模型、迁移学习等。

第2章 模型评估
08 准确率的局限性
无法正确评估不均衡的样本。
081 有什么方法解决？
使用精确率和召回率。

09 精确率与召回率的权衡
TP: 将正类预测为正类数 FP: 将负类预测为正类数 
FN: 将正类预测为负类数 TN: 将负类预测为负类数 
准确率(accuracy) = 预测对的/所有 = (TP+TN)/(TP+FN+FP+TN)
精确率(precision) = TP/(TP+FP)
召回率(recall) = TP/(TP+FN)
F1=2*P*R/(P+R)

10 平方根误差的“意外”
离群点对损失函数干扰过大。

11 什么是 ROC 曲线？
见题09，横轴FPR = FP/（FP+TN），纵轴TPR = TP/(TP+FN)

12 如何绘制 ROC 曲线？
如题11.

13 如何计算 AUC?
AUC指的是ROC曲线下的面积大小。

14 ROC 曲线相比 P-R 曲线有什么特点?
相比P-R曲线，ROC曲线有一个特点，当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。
所以，ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。但还是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R曲线则能更直观地反映其性能。

15 为什么在一些场景中要使用余弦相似度而不是欧氏距离？
余弦关注的是向量之间的角度关系，并不关心它们的绝对大小。

16 余弦距离是否是一个严格定义的距离？
三条距离公式：正定性，对称性，三角不等式
余弦距离不满足三角不等式
KL距离(相对熵，Kullback-Leibler DiverGence)，常用于两个分布之间的差异，但不满足对称性和三角不等式。
161 熵，相对熵，交叉熵的区别？
熵（评估单分布）：−∑n P(x)log(P(x))
相对熵（评估不同分布相似度）：∑x P(x)logP(x)/Q(x)
交叉熵（评估不同分布偏离度，常作为损失函数）：−∑x P(x)log(Q(x))
熵=交叉熵-相对熵

17 在对模型进行充分的离线评估后，为什么要进行在线 A/B 测试？
离线评估无法完全取代线上。

18 如何进行线上A/B测试？
实验组：新模型，对照组：旧模型。

19 如何划分实验组和对照组？
。

20 在模型评估过程中，有哪些主要的验证方法，它们的主要优缺点是什么？
随机划分成训练集和验证集
交叉检验k-fold交叉验证
留一验证
自助法：有放回的随机抽样。
21 在自助法的采样过程中，对n个样本进行n次自助抽样，当n趋于无穷大时，最终有多少数据从未被选择过？
单个样本未被抽中的概率：（1-1/n）^n=1/e = 36.8%

22 超参数有哪些调优方法？
网格搜索
随机搜索
贝叶斯优化算法：通过已知点作为先验，假设多变量高斯分布，通过最大化收获函数(Acquisition Function）来获得下一个采样点。

23 在模型评估过程中，过拟合和欠拟合具体是指什么现象？
过拟合：在训练集上表现很好，但在测试集和新数据上表现较差。
欠拟合：在训练和预测时表现都不好。

24 能否说出几种降低过拟合和欠拟合风险的方法？
降低过拟合：1.从数据入手，获取更多训练数据。2.降低模型复杂度3.正则化方法4.集成学习方法5.处理样本均衡问题
降低欠拟合：1.添加新特征（FM、GBDT、Deep-crossing）2.增加模型复杂度3.减少正则化系数

第3章 经典算法
25 在空间上线性可分的两类点，分别向SVM分类的超平面上做投影，这些点在超平面上的投影仍然是线性可分的吗？
不可分，凸包的最短距离为两个最近距离点。
250 什么是对偶函数，KKT条件？
在等式hj=0和不等式gi<=0约束条件下，求min(f)等价于引入拉格朗日算子（ai,bj）：
max_(ai,bj)[min f+∑aigi+∑bjhj] 且hj=0,gi<=0，aigi(x)=0，ai>=0

260 svm的模型函数，对偶函数，预测函数，训练误差是什么？
模型函数： min 1/2||w||^2   s.t.  yi(wxi+b)>=1
对偶函数： min 1/2∑∑aiajyiyj(xi*xj)-∑ai   s.t. ∑aiyi=0,ai>=0
预测函数：∑aiyiK(xi,x)+b    (xi,yi)为支持向量。
训练误差(hing loss)：∑j max(0,1-yj(∑aiyiK(xi,xj)+b))
26 是否存在一组参数使SVM训练误差为0？
如果有正负点在一位置则不存在。
否则：以∑aiyiK(xi,x)+b，为每个点设置一个小球体，球体内k(x1,x)为1，球体外为0。

27 训练误差为0的SVM分类器一定存在吗？
如上，可以。

280什么是带松弛变量的svm？
 min 1/2||w||^2+C∑ci   s.t.  yi(wxi+b)>=1-ci,ci>=0
28 加入松弛变量的SVM的训练误差可以为0吗？
不一定。松弛变量可容忍训练误差。

逻辑回归
29 逻辑回归相比于线性回归，有何异同？
逻辑回归处理的是分类问题，线性回归处理的是回归问题。二者都使用了极大似然估计来对训练样本进行建模。
逻辑回归的损失函数使用交叉熵，线性回归使用最小二乘法。

30 当使用逻辑回归处理多标签的分类问题时，有哪些常见做法，分别应用于哪些场景，它们之间又有怎样的关系？
单样本单标签时：多项逻辑回归（softmax regression）来进行分类。
单样本多个标签时：k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类。

决策树
31 决策树有哪些常用的启发函数？
id3：熵
c4.5:id3/信息增益比，限制分类过多的特征，因为分越散熵越小。
cart:gini。

32 如何对决策树进行剪枝？
预剪枝 1）深度 2）当前结点的样本数小于某个阈值 3）测试集的准确度提升小于阈值
后剪枝
从底层向上计算是否剪枝

第4章 降维
PCA
33 如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对PCA问题进行求解？
PCA可以找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。
PCA目标是最大化投影方差，也就是让数据在主轴上投影的方差最大。

34 PCA求解的其实是最佳投影方向，即一条直线，这与数学中线性回归的目标相近，能否从回归的角度定义PCA的目标并相应的求解问题呢？
线性判别分析
35 对于具有类别标签的数据，应当如何设计目标函数使得降维的过程中不损失类别信息？在这种目标下，应当如何进行求解？
LDA的中心思想-最大化类间距离和最小化类内距离。最终转化为一个求矩阵特征向量的问题。

36 LDA和PCA作为经典的降维算法，如何从应用的角度分析其原理的异同？从数学推导的角度，两种降维算法在目标函数上有何区别和联系？
从目标出发，PCA选择的是投影后数据方差最大的方向，LDA选择的是投影后类内方差小、类间方差大的方向，用到了类别信息。从应用的角度出发，对无监督的一般使用PCA，有监督的则应用LDA

第5章 非监督学习
K均值聚类
37 简述K均值算法的具体步骤
38 K均值算法的优缺点是什么？如何对其进行调优？
39 针对k均值算法的缺点，有哪些改进的模型？
二分 (bisecting) k-means算法
二分K-means算法首先将所有点作为一个簇，然后将簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇取决于对其进行划分是否能够最大程度的降低SSE的值。上述划分过程不断重复，直至划分的簇的数目达到用户指定的值为止。
二分k-means算法对初始质心的选择不太敏感，因为初始时只选择一个质心。
K-means++算法
K均值算法最开始随机选取数据集中K个点作为簇类中心，而K-means++算法在选择n个初始聚类中心之后，选择第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。
ISODATA算法
当K大小不确定时，可以使用ISODATA算法（迭代自组织数据分析法）。其思想是当属于某个类别的样本数过少时，把该类别去除；当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。它在K均值算法的基础上增加了两个操作，一是分裂操作，增加聚类中心；二是合并操作，对于减少聚类中心。其缺点是需要指定的参数较多：（1）预期的聚类中心数目；（2）每个类所要求的最少样本数目N；（3）最大方差Sigma；（4）两个聚类中心之间所允许最小距离D
40 证明K均值算法的收敛性
K均值算法实际上是一种最大期望算法（EM）。在E步骤中，等同于K均值算法中对于每一个点x找到当前最近的簇z。
M步骤，等同于找到最优的中心点。

高斯混合模型
41 高斯混合模型的核心思想是什么？它是如何迭代计算的？
高斯混合模型（GMM）的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，我们需要寻找每个单独的分模型的均值、方差以及权重。 采用EM算法框架来求解该优化问题。EM算法是在最大化目标函数时，先固定一个变量使整体函数变成凸优化函数，求导得到最值，然后利用最优参数更新被固定的变量，进入下一循环。具体到高斯混合模型的求解，EM算法的迭代过程如下：
首先，初始随机选择各参数的值。然后，重复下面两步，直到收敛：
（1）E步骤，根据当前参数，计算每个点由分模型生成的概率
（2）M步骤，使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。
高斯混合模型与K均值算法相同点：
1.都是可用于聚类的算法
2.都需要指定K值
3.都是使用EM算法来求解
4.都往往只能收敛于局部最优
不同点：
1.混合高斯模型可以给出一个样本属于某类的概率是多少
2.不仅仅可以用于聚类，还可以用于概率密度的估计
3.可以用于生成新的样本点

自组织映射神经网络
42 自组织映射神经网络是如何工作的？它与K均值算法有何区别？
自组织映射神经网络（SOM）本质上是一个两层的神经网络，包含输入层与输出层

43 怎样设计自组织映射神经网络并设定网络训练参数？
输出层神经元的数量
设计输出层节点的排列
初始化权值
设计拓扑领域
设计学习率

44 以聚类算法为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？
估计在数据集上进行聚类的可行性，以及聚类方法产生结果的质量。
估计聚类趋势
检测数据分布中是否存在非随机的簇结构。可以观察聚类误差是否随聚类类别数量的增加而单调变化，如果数据是随机分布的，那么其变化幅度应该较不显著，并且也找不到一个合适的K对应数据的真实簇数。另外，也可以应用霍普金斯统计量来判断数据在空间上的随机性。
判断数据簇类
例如手肘法和Gap Statistics方法
测定聚类质量
考察簇的分离情况和簇的紧凑情况来评估聚类的效果。测量指标有：
轮廓系数
均方根标准偏差
R方
改进的HubertT统计
第6章 概率图模型
45 贝叶斯网络的联合概率分布
46 马尔科夫网络的联合概率分布
47 解释贝叶斯模型的原理，并给出概率图模型表示
48 解释最大熵模型的原理，并给出概率图模型表示
49 常见的概率图模型中，哪些是生成式模型，哪些是判别式模型？
生成式模型是由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型
典型的生成式模型有：朴素贝叶斯、贝叶斯网络、pLSA、LDA、隐马尔科夫模型
判别式模型是由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型
典型的判别式模型有：最大熵模型、条件随机场

50 如何对中文分词问题用隐马尔科夫模型进行建模和训练？
隐马尔科夫模型通常用来解决序列标注问题，因此也可以将分词问题转化为一个序列标注问题来进行建模。例如，可以对中文句子中每个字做以下标注，B表示一个词开头的第一个字，E表示一个词结尾的最后一个字，M表示一个词中间的字，S表示一个单字词，则隐状态的取值空间为{B,E,M,S}。同时对隐状态的转移概率可以给出一些先验知识，B和M后面只能是M或者E，S和E后面只能是B或者S。而每个字就是模型中的观测状态，取值空间为预料中的所有中文字。完成建模之后，使用语料进行训练可以分为有监督训练和无监督训练。

51 最大熵马尔可夫模型（MEMM）为什么会产生标注偏置问题？如何解决？
由于局部归一化的影响，隐状态会倾向于转移到后续状态可能更少的状态上，以提高整体的后验概率。这就是标注偏置问题。
可以使用条件随机场（CRF），进行全局归一化。

52 常见的主题模型有哪些？试介绍其原理。
基于词袋模型或N-gram模型的文本表示模型有一个明显的缺陷，无法识别出两个不同的词或词组具有相同的主题。因此，可以通过主题模型将具有相同主题的词或词组映射到同一维度上。
主题模型是用来在大量文档中发现潜在主题的一种统计模型。

pLSA
LDA
53 如何确定LDA模型中的主题个数？
常用评估指标为困惑度（perplexity），在文档集合D上，模型的困惑度被定义为：

perplexity(D)=\exp{\{ -\frac{\sum_{d=1}^M \log p(w_d)}{\sum_{d=1}^M N_d} \}}
其中，M为文档的总数，wd为文档d中单词所组成的词袋向量，p(wd)是模型所预测的文档d的生成概率，Nd是文档d中单词的总数。
在实践中，一般选择合理范围内困惑度的下降明显变慢（拐点）的时候。

54 如何用主题模型来解决推荐系统中的冷启动问题？
推荐系统中的冷启动问题，是指没有大量用户数据的情况下如何给用户进行个性化推荐，目的是最优化点击率、转化率或用户体验。冷启动问题一般分为用户冷启动、物品冷启动和系统冷启动三大类。
解决冷启动问题的方法一般是基于内容的推荐。对于用户冷启动，可以根据用户的注册信息，得到用户的兴趣主题，找到与该用户兴趣相同的其他用户，通过他们的历史行为来预测用户的兴趣。对于物品冷启动，可以根据电影的导演、演员等信息推测电影所属于的主题，如何基于主题向量找到相似的电影，并将新电影推荐给以往喜欢看这些相似电影的用户。
解决系统冷启动，首先可以得到每个用户和电影对应的主题向量。当系统没有任何数据时，我们需要一些先验知识来指定，并且由于主题的数目通常比较小，随着系统的上限，收集到少量的数据之后我们就可以对主题之间的偏好程度得到一个比较准确的估计。

第7章 优化算法
55 有监督学习设计的损失函数有哪些？请列举并简述它们的特点？
损失函数（Loss Function）：是定义在单个样本上的，是指一个样本的误差。
代价函数（Cost Function）：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均。
目标函数（Object Function）：是指最终需要优化的函数，一般来说是经验风险+结构风险，也就是（代价函数+正则化项）。
常用的损失函数：

0-1损失函数（0-1 loss function）
L(y,f(x))={1,y≠f(x)0,y=f(x)
对数损失函数（logarithmic loss function）
L(y,f(x))=−logp(y|x)
指数损失函数
L(y,f(x))=exp(−yf(x))
Hinge(合页)损失函数
L(w,b)=max0,1−yf(x)

其中y=+1或y=-1，f(x)=wx+b，当为SVM的线性核时。
平方损失函数（quadratic loss function）
L(y,f(x))=(y−f(x))2
绝对值损失函数（absolute loss function）
L(y,f(x))=|y−f(x)|
常用的代价函数：
均方误差（Mean Squared Error）
MSE=1m∑i=1m(yi−y^i)2

平均绝对误差（Mean Absolute Error）
MAE=1m∑i=1m|yi−y^i|

均方根误差
RMSE=1m∑i=1m(yi−y^i)2−−−−−−−−−−−−−√

交叉熵代价函数（Cross Entry）
L(w,b)=−1N∑i=1m(yilogf(xi)+(1−yi)log(1−f(xi)))

56 机器学习中的优化问题，哪些是凸优化问题，哪些是非凸优化问题？
凸优化问题包括支持向量机、线性回归、逻辑回归等线性模型，非凸优化问题包括低秩模型（如矩阵分解）、深度神经网络模型等。

57 无约束优化问题的优化方法有哪些？
经典的优化算法可以分为直接法和迭代法两大类。
直接法，就是能够直接给出优化问题最优解的方法。它要求目标函数满足两个条件，L()是凸函数，且有闭式解。以线性回归为例
线性回归中，其代价函数为：
J(θ)=12∑i=1m(hθ(x(i))−y(i))2

使用矩阵对损失函数求导，有：
L(w)===12(XW−y)T(XW−y)12(WTXTXW−WTXTy−yTXW+yTy)12(WTXTXW−2WTXTy+yTy)

则，
∂L(w)∂w=12[WTXTX−2XTy]=0

XTXW=XTy

得
W=(XTX)−1XTy

迭代法又分为一阶法和二阶法两类。一阶法对函数做一阶泰勒展开；又称梯度下降法；
二阶法做二阶泰勒展开；又称牛顿法。二阶法收敛速度快，但在高维情况下，海森矩阵求逆的计算复杂度很大，而且当目标为非凸时，可能会收敛到鞍点。

58 如何验证求目标函数梯度功能的正确性？
可以做梯度验证。

59 当训练数据量特别大时，经典的梯度下降法存在什么问题，需要做如何改进？
经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。当M很大时，这需要很大的计算量，耗费很长的计算时间。可采用随机梯度下降法或者小批量梯度下降法。随机梯度下降法是使用单个数据对模型参数进行更新，大大加快了收敛速率，也适用于在线更新的场景。小批量梯度下降法是同时使若干个训练数据。有三点需要注意的地方：
1）如何选取参数m？一般通过调参。取m为2的幂次，充分利用矩阵运算操作。
2）如何挑选m个训练数据？需要先对所有的数据进行随机排序
3）如何选取学习速率a？为了加快收敛速率，同时提高求解精度，通常会采用衰减学习速率的方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整。

60 随机梯度下降法为什么会失效？
因为随机梯度下降每步仅仅随机采样一个（或少量）样本来估计当前梯度，计算速度快，内存开销小。但由于每步接受的信息量有限，随机梯度下降达对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛的情况。对随机梯度下降来说，如何徘徊在山谷和鞍点。其中，在山谷中，准确的梯度方向是沿着山道向下，稍有偏离就会撞向山壁，而粗糙的梯度估计使得它在两山壁间来回反弹震荡，不能沿山道方向迅速下降，导致收敛不稳定和收敛速度慢。在鞍点处，随机梯度下降法会进入一片平坦的地方（plateau），梯度几乎为零，随机梯度下降法无法准确察觉出梯度的微小变化，造成停滞。

61 如何改进？
动量（Momentun）法
AdaGrad法
Adam法
62 L1正则化使得模型参数具有稀疏性的原理是什么？
事实上，带正则项与带约束条件是等价的。为了约束w的可能取值空间从而防止过拟合，我们为最优化问题加上一个约束，就是w的L2范数的平方不能大于m：
{min∑Ni=1(yi−wTxi)2s.t.||w||2≤m

L2正则化相当于为参数定义了一个圆形的解空间（因为必须保证L2范数不能大于m），而L1正则化相当于为参数定义了一个菱形的解空间。
我们令黄色部分是L2和L1正则约束后的解空间，等高线是凸优化问题中目标函数的等高线，可知L1正则化多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。

第8章 采样
63 举例说明采样在机器学习中的应用。
采样本质是对随机现象的模拟，根据给定的概率分布，来模型产生一个对应的随机事件。

可以通过采样来用较少量的样本点来近似总体分布，并刻画总体分布中的不确定性。可以简化问题。
对当前数据进行重采样，可以充分利用已有的数据集，挖掘更多的信息。例如，处理样本不均衡问题。
利用采样方法进行随机模拟，从而对复杂模型进行近似求解和推理。例如，在LDA（隐狄利克雷模型）中，采用吉布斯采样对隐变量的分布进行采样。
64 如何编程实现均匀分布随机数生成器？
采用线性同余法，生成离散均匀分布伪随机数，计算公式为：
xt+1=a∗xt+c(mod m)

可得到【0,m-1】上的随机整数。除以m可得0-1区间上的连续随机数。

65 通用采样方法主要思想以及具体操作步骤
逆变换采样。其过程为：
1）从均匀分布U(0,1)产生一个随机数；
2）计算xi=Φ−1(ui)，其中Φ−1是累计分布函数的逆函数。
拒绝采样
对于目标分布p(x)，选取一个容易采样的参考分布q(x)，使得对于任意x都有p(x)<=Mq(x)，则过程为:
1）从参考分布q(x)中随机抽取一个样本x
2）从均匀分布U(0,1)产生一个随机数u
3）如果ui<p(xi)Mq(xi)，则接受样本x；否则拒绝，重新进行步骤1-3，直到新产生的样本被接受。
66 如何对高斯分布进行采样？
逆变换采样
拒绝采样
67 简述MCMC采样法的主要思想
MCMC采样的基本思想是：针对待采样的目标分布，构造一个马尔可夫链，使得该马尔科夫链的平稳分布就是目标分布；然后，从任何一个初始状态出发，沿着马尔科夫链进行状态转移，最终得到的状态转移序列会收敛到目标分布，由此可以得到目标分布的一系列样本。

68 几种常见的MCMC采样法
Metropolis-Hastings采样法
吉布斯采样法
其核心思想是每次只对样本的一个维度进行采样和更新
69 MCMC采样法如何得到相互独立的样本
可以运行多条马尔科夫链，这样不同链上的样本是独立的；
或者在同一天马尔科夫链上每隔若干个样本才取一个，这样就是近似独立的。

70 如何对贝叶斯网络进行采样？如果只需要考虑一部分变量的边缘分布，如何采样？如果网络中含有观测变量，又该如何采样？
71 对于二分类问题，当训练集中负样本非常不均衡时，如何处理数据以更好地训练分类模型？
基于数据的方法
对数据进行重采样，使原本不平衡的样本变得均衡。例如，随机采样（过采样和欠采样）。随机过采样是从少数类样本集中随机重复抽取样本（有放回）得到更多样本；随机欠采样则相反，从多数类样本集中随机选取较少的样本（有放回和无放回）。
但是，直接的过采样扩大了数据规模，增加了模型训练的复杂度，同时容易造成过拟合；欠采样会丢弃一些样本，可能会损失部分有用信息，造成模型只学到了整体模式的一部分。为了解决以上问题，可以使用SMOTE算法生成新的样本。SMOTE算法对少数类集中每个样本x，从它在少数类的K近邻中随机选一个样本y，然后在x，y连续上随机选取一点作为新合成的样本。另外有，Borderline-SMOTE、ADASYN等改进算法。此外，还可以采用一些数据处理方法（如基于Tomek Links）来进一步降低合成样本带来的类间重叠，更好地训练分类器。
对于欠采样，可以使用Informed Undersampling来解决数据丢失问题，如：Easy Ensemble算法/Balance Cascade算法/NearMiss/One-sided Selection 算法。
基于算法的方法
可以通过改变模型训练时的目标函数（如代价敏感学习中不同类别有不同的权重）来矫正不平衡性；当样本数目极其不平衡时，也可以将问题转化为单类学习（one-class learning）、异常检测（anomaly detecion）。
第9章 前向神经网络
72 写出常用激活函数以及导数
sigmid
f(z)=11+exp(−z)

对应的导函数为：
f′(z)=f(z)(1−f(z))
tanh
f(z)=tanh(z)=ez−e−zez+e−z

对应的导函数为：
f′(z)=1−(f(z))2
relu
f(z)=max(0,z)

对应的导函数为：
f′(z)={1,z>00,z≤0
73 为什么sigmoid和tanh激活函数会导致梯度消失的现象？
sigmoid激活曲线可以看出：当z很大时，f(z)趋近于1；当z很小时，f(z)趋近于0。其导数在z很大或很小都会趋近于0，造成梯度消失的现象。
sigmoid激活曲线可以看出：当z很大时，f(z)趋近于1；当z很小时，f(z)趋近于-1。其导数在z很大或很小都会趋近于0，造成梯度消失的现象。实际上，tanh激活函数相当于sigmoid的平移。

74 Relu系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？
优点：
从计算上，sigmoid和tanh激活函数均需要计算指数，复杂度高，而Relu只需要一个阈值就可以得到激活值。
relu的非饱和性可以有效地解决梯度消失的问题，提供相对宽的激活边界。
relu的单侧拟制提供了网络的稀疏表达能力。
局限性：
其训练过程中会导致神经元死亡的问题。这是由于relu函数导致负梯度在经过该relu单元时被置为0，且在之后也不被任何数据激活，导致梯度永远为0。在实际训练中，如果学习率设置过大，会导致一定比率的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。
所以，有relu的变种Leaky Relu，既实现了单侧拟制，又保留了部分负梯度信息以致不完全丢失，但另一方面，a值的选择增加了问题难度，又有参数化的PReLU，LReLU，Random ReLU等。

75 relu为什么不能根治梯度消失？
梯度反向传播不是只和激活函数有关

76 写出多层感知机的平方误差和交叉熵损失函数。推导各层参数更新的梯度计算公式。
多层感知机中，输入信号通过各个网络层的隐节点产生输出的过程称为前向传播。我们定义低(l)层的输入为x(l),输出为a(l)；在每一层中z(l)=w(l)x(l)+b(l),，然后利用激活函数f得到a(l)=f(z(l))；a(l)直接作为下一层输入，即x(l+1)。设z(l)和a(l)为sl的向量，则Wl为sl∗sl+1维的矩阵。
平方误差的整体代价函数为：
J(W,b)=[1m∑i=1mJ(W,b;x(i),y(i))]+λ2∑l=1N∑i=1Sl−1∑j=1Sl(W(l)ji)2=[1m∑i=1m12||y(i)−o(i)||2]+λ2∑l=1N∑i=1Sl−1∑j=1Sl(W(l)ji)2

在二分类中，交叉熵的整体代价函数为：
J(W,b)=[1m∑i=1mJ(W,b;x(i),y(i))]+λ2∑l=1N∑i=1Sl−1∑j=1Sl(W(l)ji)2=−[1m∑i=1my(i)lno(i)+(1−y(i))ln(1−o(i))]+λ2∑l=1N∑i=1Sl−1∑j=1Sl(W(l)ji)2

在多分类下，
J(W,b)=−[1m∑i=1m∑k=1ny(i)klno(i)k]+λ2∑l=1N∑i=1Sl−1∑j=1Sl(W(l)ji)2

梯度下降法中每次迭代对参数W（网络连接权重）和b(偏置）进行更新：
W(l)ji=W(l)ji−α∂∂W(l)jiJ(w,b)b(l)j=b(l)j−α∂∂b(l)jJ(w,b)

其中，α为学习率。
其中：
∂∂W(l)jiJ(w,b)=∂J(w,b)∂z(l)j∂z(l)j∂W(l)ji

先计算损失函数对隐含层的偏导
∂J(w,b)∂z(l)j=∑k=1Sl+1(∂J(w,b)∂z(l+1)k∂z(l+1)k∂z(l)j)

∂z(l+1)k∂z(l)j=∂(∑Slj=1(W(l+1)kjx(l+1)j+b(l+1)k))∂z(l)j

其中b(l+1)k与z(l)j无关可以省去，x(l+1)=a(l)=f(z(l)),因此
∂z(l+1)k∂z(l)j=W(l+1)kjf′(z(l)j)

∂J(w,b)∂z(l)j可以看做损失函数在第l层第i个节点产生的残差量，记为δ(l)j，从而递推公式可以表示为：
δ(l)j=(∑k=1Sl+1W(l+1)kjδ(l+1)k)f′(z(l)j)

损失对参数函数的梯度可以写为：
∂∂W(l)jiJ(W,b)=∂J(w,b)∂z(l)j∂z(l)j∂W(l)ji=δ(l)jx(l)ia(l−1)i

∂∂b(l)jJ(W,b)=δ(l)j

下面针对不同的损失函数计算最后一层的残差δ(L)；得到δ(L)之后其他层的残差可以根据递推公式计算。
平方误差损失：
J(W,b)=12||y(i)−o(i)||2δ(L)=−(y−a(L))f′(z(L))

交叉熵损失：
J(W,b)=−∑k=1nyklno(L)kδ(L)=a(L)k−yk

77 平方误差和交叉熵损失函数分别适合什么场景？
一般来说，平方损失函数更适合输出为连续，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失则更适合二分类或多分类的场景。
因为平方损失函数相对于输出层的导数为：
δ(L)=−(y−a(L))f′(z(L))

当激活函数为sigmoid时，如果z的绝对值较大，函数的梯度会趋于饱和，即导数绝对值非常小，导致残差也小，使得基于梯度的学习速度非常缓慢。当使用交叉熵损失函数时，相对于输出层的导数（残差）为
δ(L)=a(L)k−yk

此时的导数是线性的，因此不会存在学习速度过慢的问题。

78 卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性及其作用。
稀疏交互（Sparse Interaction）
在卷积神经网络中，卷积核尺度远小于输入的维度，使得每个输出神经元仅与前一层局部区域内的神经元存在连接权重。
它的物理意义是，通常图像、文本、语言等现实世界中的数据都具有局部的特征结构，可以先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。
参数共享（Parameter sharing）
参数共享是指在同一个模型中的不同模块中使用相同的参数。在卷积神经网络中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。根据参数共享的思想，我们只需要学习一组参数组合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的存储需求。
它的物理意义是使得卷积层具有平移不变性，也就是说神经网络的输出对于平移变换来说应当是等变的。
79 常见的池化操作有哪些？池化的作用是什么？
均值池化
对邻域内特征数值求平均，能够拟制由于邻域大小受限造成估计值方差增大的现象，特点是对背景的保留效果好。
最大池化
通过取邻域内特征的最大值来实现，能够拟制网络参数误差造成估计均值偏移的现象，特点是更好地提取纹理信息。
相邻重叠区域的池化
是采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠的区域。
金字塔池化
考虑了多尺度信息的描述，例如同时计算1x1，2x2，4x4的矩阵的池化并将结果拼接在一起作为下一网络层的输入。
池化操作能显著降低参数量，并且能保持对平移、伸缩、旋转操作的不变性。
80 卷积神经网络如何用于文本分类任务？
卷积神经网络的核心思想是捕捉局部特征，能够自动地对N-gram特征进行组合和筛选，获得不同抽象层次的语义信息。
卷积神经网络有输入层、卷积层、池化层、输出层。
（1）输入层是一个NxK的矩阵，其中N为文章所对应的单词总数，K是每个词对应的表示向量的维度。每个词的K维向量可以是预先在其他语料库中训练好的，也可以作为未知的参数有网络训练得到。这两种方法各有优势，一方面，预先训练的词嵌入可以利用其他语料库得到更多的先验知识；另一方面，由当前网络训练的词向量能够更好地抓住与当前任务相关联的特征。
（2）第二次维卷积层，定义不同大小的滑动窗口进行卷积操作
（3）第三层为池化层
（4）接入一个全连接层，使用softmax激活函数输出每个类别的概率。

81 ResNet的提出背景和核心理论是什么？
ResNet的提出背景是解决或缓解生成的神经网络训练中的梯度消失问题。
根据误差传播公式：
δ(l)i=(∑j=1Sl+1W(l+1)jiδ(l+1)j)f′(z(l)i)

将式再展开一层，可以得到；
δ(l)i=(∑j=1Sl+1W(l+1)ji(∑k=1Sl+2W(l+2)kjδ(l+2)kf′(z(l+1)j))f′(z(l)i)

可以看到误差传播可以写成参数W(l+1)ji、W(l+2)kj以及导数f′(z(l+1)j)、f′(z(l)i)连乘的形式，当误差由L层传播到除输入以外的第一个隐含层的时候，会涉及到非常多的参数和导数的连乘，这时候误差很容易产生消失或者膨胀，影响该层参数的正确学习，因此深层神经网络的拟合和泛化能力较差。
ResNet通过调整网络结构来解决上述问题。将输入短接到两层之后，可以直接学习一个恒等映射，可以有效改善深层的神经网络学习问题。

第10章 循环神经网络
82 处理文本数据时，循环神经网络与前馈神经网络相比有什么特点?
传统文本处理任务的方法中一般讲TF-IDF向量作为特征输入，这样的表示实际上丢失了输入的文本序列中每个单词的顺序。卷积神经网络对文本建模时，输入变长的字符串或者单词串，然后通过滑动窗口加池化的方式将原先的输入转换成一个固定长度的向量表示，这样做可以捕捉到原文本中的一些局部特征，但是两个单词之间的长距离依赖关系还是很难被学习到。
一个长度为T的序列用循环神经网络建模，展开之后可以看做是一个T层的前馈神经网络。其中第t层的隐含状态h(t)编码了序列中前t个输入的信息，可以通过当前的输入x(t)和上一层神经网络状态h(t-1)计算得到；最后一层的状态h(T)编码了整个序列的信息，因此可以作为整篇文档的压缩表示，以此为基础的结构可以应用于多种具体任务。例如，在h(T)后面直接接一个Softmax层，输出文本所属类别的预测概率y，就可以实现文本分类。

83 循环神经网络为什么会出现梯度消失或梯度爆炸？有哪些改进方案？
10 平方根误差的“意外”
RMSE计算公式为：

RMSE=\sqrt{ \frac{(y-\hat y)^2}{n}}
在实际问题中，如果存在个别偏离程度非常大的离群点时，即使离群点数量非常少，也会让RMSE指标变得很差。
思考这个问题有三个方面：第一，如果我们认为这些离群点是噪声点的话，就需要在数据预处理的阶段把这些噪声过滤掉；第二，如果不认为是噪声点，就需要进一步提高模型的预测能力，将离群点产生的机制建模进去；第三，可以找一个更合适的指标来评估该模型，例如平均绝对百分比误差（MAPE）(Mean Absolute Percent Error)

